{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    Task, LearnerType, Params,\n",
    "    get_AutoSklearnClassifier, get_XGBModel, \n",
    "    AutoSklearnClassifier, XGBClassifier,\n",
    "    dump_pkl, dump_json, load_pkl, load_json\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IDS = [\n",
    "    (\"credit-approval/credit-rating\", 29),\n",
    "    (\"credit-g\", 31),\n",
    "    (\"diabetes/pima-diabetes\", 37),\n",
    "    (\"spambase\", 44),\n",
    "    (\"tic-tac-toe\", 50),\n",
    "    (\"electricity\", 151),\n",
    "    (\"pc4\", 1049),\n",
    "    (\"pc3\", 1050),\n",
    "    (\"JM1\", 1053),\n",
    "    (\"KC2\", 1063),\n",
    "    (\"kc1\", 1067),\n",
    "    (\"pc1\", 1068),\n",
    "    (\"bank-marketing/bank-marketing-full\", 1461),\n",
    "    (\"blood-transfusion-service-center\", 1464),\n",
    "    (\"ilpd\", 1480),\n",
    "    (\"madelon\", 1485),\n",
    "    (\"ozone-level-8hr\", 1487),\n",
    "    (\"phoneme\", 1489),\n",
    "    (\"qsar-biodeg\", 1494),\n",
    "    (\"cylinder-bands\", 6332),\n",
    "    (\"dresses-sales\", 23381),\n",
    "    (\"churn\", 40701),\n",
    "    (\"climate-model-simulation-crashes\", 40994),\n",
    "]\n",
    "\n",
    "\n",
    "DATASETS = []\n",
    "for dataset_name, dataset_id in IDS:\n",
    "    data = fetch_openml(data_id=dataset_id, parser=\"auto\", as_frame=True)\n",
    "    assert len(data.target_names) == 1\n",
    "    target = data.target_names[0]\n",
    "\n",
    "    if data.frame.shape[0] < 1000:\n",
    "        continue\n",
    "    \n",
    "    if len(data.frame[target].unique()) != 2:\n",
    "        continue\n",
    "\n",
    "    X = data.frame.drop(columns=[target])\n",
    "    (l1, l2), (c1, c2) = np.unique(data.frame[target], return_counts=True)\n",
    "    (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
    "    # NOTE: convention that minority label will always be 1\n",
    "    y = data.frame[target] == l1\n",
    "    DATASETS.append((dataset_name, X, y))\n",
    "\n",
    "\n",
    "[name for name, _, _ in DATASETS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import perf_counter\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score, \n",
    "    fbeta_score, \n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "def validate(X, y, model, task, name, ratio):\n",
    "    if not Params.VALIDATE:\n",
    "        return\n",
    "\n",
    "    if isinstance(model, AutoSklearnClassifier):\n",
    "        model = model.get_models_with_weights()[0][1]\n",
    "        _, clf = model.steps.pop()\n",
    "    elif isinstance(model, XGBClassifier):\n",
    "        clf = model\n",
    "        model = object()\n",
    "        model.steps = []\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    res = load_json(Params.BASE_LEARNER, task, name, ratio)\n",
    "    skf = StratifiedKFold(n_splits=Params.K_FOLDS, shuffle=True, random_state=Params.SEED)\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        if str(fold) in res:\n",
    "            continue\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        for _, step in model.steps:\n",
    "            if hasattr(step, \"fit_resample\"):\n",
    "                X_train, y_train = step.fit_resample(X_train, y_train)\n",
    "            elif hasattr(step, \"fit_transform\"):\n",
    "                X_train = step.fit(X_train, y_train)\n",
    "            elif hasattr(step, \"fit\") and hasattr(step, \"transform\"):\n",
    "                X_train = step.fit(X_train, y_train).transform(X_train)\n",
    "            else:\n",
    "                assert False, f\"This step is not a transformer or resampler: {step}.\"\n",
    "\n",
    "        train_time = perf_counter()\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        train_time = perf_counter() - train_time\n",
    "\n",
    "        inference_time = perf_counter()\n",
    "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "        inference_time = perf_counter() - inference_time\n",
    "\n",
    "        if task == Task.DATA_IMBALANCE:\n",
    "            y_test_threshold, y_test, y_prob_threshold, y_prob = train_test_split(\n",
    "                y_test, y_prob, test_size=0.5, random_state=Params.SEED, stratify=y_test\n",
    "            )\n",
    "            fpr, tpr, thresholds = roc_curve(y_test_threshold, y_prob_threshold)\n",
    "            y_pred = thresholds[(3*tpr*(1-fpr)/(2*(1-fpr)+tpr)).argmax()] < y_prob\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        res[fold] = {\n",
    "            \"train_time\": train_time,\n",
    "            \"inference_time\": inference_time,\n",
    "\n",
    "            \"auc_roc\": roc_auc_score(y_test, y_prob),\n",
    "            \"confusion_matrix\": [int(n) for n in confusion_matrix(y_test, y_pred).ravel()],\n",
    "\n",
    "            \"minority_precision\": precision_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
    "            \"majority_precision\": precision_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
    "\n",
    "            \"minority_recall\": recall_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
    "            \"majority_recall\": recall_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
    "\n",
    "            \"minority_f1\": f1_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
    "            \"majority_f1\": f1_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
    "            \"macro_f1\": f1_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "\n",
    "            \"minority_f2\": fbeta_score(y_test, y_pred, beta=2, pos_label=1, average='binary', zero_division=0.0),\n",
    "            \"majority_f2\": fbeta_score(y_test, y_pred, beta=2, pos_label=0, average='binary', zero_division=0.0),\n",
    "            \"macro_f2\": fbeta_score(y_test, y_pred, beta=2, average='macro', zero_division=0.0),\n",
    "        }\n",
    "\n",
    "    dump_json(res, Params.BASE_LEARNER, task, name, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "def sorted_class_count(y):\n",
    "    (l1, l2), (c1, c2) = np.unique(y, return_counts=True)\n",
    "    (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
    "    return c1, c2\n",
    "\n",
    "IMBALANCED_DATSETS = []\n",
    "ratios = [0.5, 0.25] + [r/100 for r in range(1, 21)]\n",
    "for name, X, y in DATASETS:\n",
    "    c1, c2 = sorted_class_count(y)\n",
    "    IMBALANCED_DATSETS.append((name, c1/c2, X, y))\n",
    "    for ratio in sorted(ratios, reverse=True):\n",
    "        c1, c2 = sorted_class_count(y)\n",
    "        new_minority_count = int(c2 * ratio)\n",
    "        if c1 < new_minority_count:\n",
    "            continue\n",
    "        X, y = make_imbalance(X, y, sampling_strategy={0: c2, 1: new_minority_count}, random_state=Params.SEED)\n",
    "        IMBALANCED_DATSETS.append((name, ratio, X, y))\n",
    "\n",
    "for _, ratio, _, y in IMBALANCED_DATSETS:\n",
    "    c1, c2 = sorted_class_count(y)\n",
    "    assert (c1 / c2 - ratio) < 0.001\n",
    "\n",
    "[(name, ratio) for name, ratio, _, _ in IMBALANCED_DATSETS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for name, ratio, X, y in IMBALANCED_DATSETS:\n",
    "    try:\n",
    "        model = load_pkl(Params.BASE_LEARNER, Task.DATA_IMBALANCE, name, ratio)\n",
    "    except:\n",
    "        model = get_AutoSklearnClassifier(X, y, Task.DATA_IMBALANCE)\n",
    "        dump_pkl(model, Params.BASE_LEARNER, Task.DATA_IMBALANCE, name, ratio)\n",
    "    validate(X, y, model, Task.DATA_IMBALANCE, name, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "NOISY_DATASETS = []\n",
    "noise_amount = [a / 100 for a in range(1, 11)]\n",
    "\n",
    "for name, X, y in DATASETS:\n",
    "    NOISY_DATASETS.append((name, 0, X.copy(), y.copy()))\n",
    "    noise_to_add = int(len(X) / 100)\n",
    "    indices_left = [X.index.copy() for _ in range(X.shape[1])]\n",
    "    for noise in sorted(noise_amount):\n",
    "        for i, feature in enumerate(X.columns):\n",
    "            noise_indices = np.random.choice(indices_left[i], noise_to_add, replace=False)  \n",
    "            if X[feature].dtype == \"float64\":\n",
    "                X.loc[noise_indices, feature] = np.random.uniform(X[feature].min(), X[feature].max(), noise_to_add)\n",
    "            elif X[feature].dtype == \"int64\":\n",
    "                X.loc[noise_indices, feature] = np.random.randint(X[feature].min(), X[feature].max()+1, noise_to_add)\n",
    "            elif X[feature].dtype == \"category\":\n",
    "                X.loc[noise_indices, feature] = np.random.choice(X[feature].unique(), noise_to_add)\n",
    "            else:\n",
    "                assert False, X[feature].dtype\n",
    "            indices_left[i].drop(noise_indices)\n",
    "\n",
    "        NOISY_DATASETS.append((name, noise, X.copy(), y.copy()))\n",
    "\n",
    "\n",
    "for i in range(1, len(NOISY_DATASETS)):\n",
    "    name, noise, X2, y2 = NOISY_DATASETS[i]\n",
    "    _, _, X, y = [\n",
    "        (name2, noise2, X2, y2) \n",
    "        for name2, noise2, X2, y2 in NOISY_DATASETS \n",
    "        if name2 == name and noise2 == 0\n",
    "    ][0]\n",
    "    for feature in X.columns:\n",
    "        assert len(X[feature]) == len(X2[feature])\n",
    "        sm = sum(a != a2 for a, a2 in zip(X[feature], X2[feature]))\n",
    "        diff = 0.5 if len(X[feature].unique()) <= 10 else 0.02\n",
    "        assert abs(sm / len(X[feature]) - noise) < diff, f\"{sm / len(X[feature])} {noise} {len(X[feature].unique())}\"\n",
    "\n",
    "\n",
    "[(name, noise) for name, noise, _, _ in NOISY_DATASETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class NoiseRemover:\n",
    "    def fit_resample(self, X, y):\n",
    "        model = get_XGBModel(X, y, Params.BASE_LEARNER, LearnerType.REGRESSION)\n",
    "        model = model.fit(X, y)\n",
    "        X_transformed, y_transformed = X, model.predict(X)\n",
    "        return X_transformed, y_transformed\n",
    "\n",
    "\n",
    "for name, noise, X, y in NOISY_DATASETS:\n",
    "    X_transformed, y_transformed = NoiseRemover().fit_transform(X, y)\n",
    "    model = get_XGBModel(X_transformed, y_transformed, Params.BASE_LEARNER, LearnerType.CLASSIFICATION)\n",
    "    validate(X_transformed, y_transformed, model, Task.NOISY_DATA, name, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "HIDDEN_DATASETS = []\n",
    "hidden_amount = [a / 100 for a in range(1, 11)]\n",
    "\n",
    "for name, X, y in DATASETS:\n",
    "    X_hidden = pd.DataFrame(np.empty((0, X.shape[1])))\n",
    "    HIDDEN_DATASETS.append((name, 0, X.copy(), X_hidden.copy(), y.copy()))\n",
    "    num_to_hide = int(len(X) / 100)\n",
    "    indices_left = X.index\n",
    "    for hidden in sorted(hidden_amount):\n",
    "        indices_to_hide = np.random.choice(indices_left, num_to_hide, replace=False)        \n",
    "        X_hidden = pd.concat([X_hidden, X.loc[indices_to_hide]], ignore_index=True)\n",
    "        indices_left = indices_left.drop(indices_to_hide)\n",
    "        X = X.drop(indices_to_hide)\n",
    "        y = y.drop(indices_to_hide)\n",
    "        HIDDEN_DATASETS.append((name, hidden, X.copy(), X_hidden.copy(), y.copy()))\n",
    "\n",
    "for _, hidden, X, X_hidden, y in HIDDEN_DATASETS:\n",
    "    assert abs(len(X_hidden) / len(X) - hidden) < 0.015, f\"{len(X_hidden) / len(X)}, {hidden}\"\n",
    "    assert len(X) == len(y)\n",
    "\n",
    "\n",
    "[(name, hidden) for name, hidden, _, _, _ in HIDDEN_DATASETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "for name, hidden, X, X_hidden, y in HIDDEN_DATASETS:\n",
    "    try:\n",
    "        X, y = load_pkl(Params.BASE_LEARNER, Task.SEMI_SUPERVISED, name, hidden)\n",
    "    except:\n",
    "        num_indices_to_select = len(X_hidden) // 10\n",
    "        while len(X_hidden) != 0:\n",
    "            model = BaggingClassifier(\n",
    "                estimator=get_XGBModel(X, y, Params.BASE_LEARNER, LearnerType.CLASSIFICATION),\n",
    "                n_estimators = 11,\n",
    "                max_samples = 0.5,\n",
    "                max_features = 0.5,\n",
    "                bootstrap = True,\n",
    "                bootstrap_features = True,\n",
    "                n_jobs=1,\n",
    "                random_state = Params.SEED\n",
    "            ).fit(X, y)\n",
    "            y_prob = model.predict_proba(X_hidden)[:,1]\n",
    "            max_indices = np.argsort(np.maximum(y_prob, 1 - y_prob))[::-1][:num_indices_to_select]\n",
    "            X = pd.concat([X, pd.DataFrame(X_hidden.iloc[max_indices])], ignore_index=True)\n",
    "            y = pd.concat([y, pd.DataFrame(0.5 <= y_prob[max_indices])], ignore_index=True)\n",
    "            X_hidden = X_hidden.drop(max_indices) \n",
    "        dump_pkl((X, y), Params.BASE_LEARNER, Task.SEMI_SUPERVISED, name, hidden) \n",
    "\n",
    "    model = get_XGBModel(X, y, Params.BASE_LEARNER, LearnerType.CLASSIFICATION)\n",
    "    validate(X, y, model, Task.SEMI_SUPERVISED, name, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inadequate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, X, y in DATASETS:\n",
    "    try:\n",
    "        model = load_pkl(Params.BASE_LEARNER, Task.FEATURE_INADEQUACY, name, None)\n",
    "    except:\n",
    "        model = get_AutoSklearnClassifier(X, y, task=Task.INADEQUATE_FEATURES)\n",
    "        dump_pkl(model, Params.BASE_LEARNER, Task.DATA_IMBALANCE, name, ratio)\n",
    "    validate(X, y, model, Task.FEATURE_INADEQUACY, name, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
