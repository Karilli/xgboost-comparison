{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of auto-sklearn version that supports SMOTE, source has to be downloaded localy\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.insert(0, \"../my_autosklearn\")\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "K_FOLDS = 5\n",
    "BASE_LEARNER = \"RF\"\n",
    "\n",
    "\n",
    "class Task(Enum):\n",
    "    DATA_IMBALANCE = 1\n",
    "    FEATURE_INADEQUACY = 2\n",
    "    SEMI_SUPERVISED = 3\n",
    "    NOISY_DATA = 4\n",
    "\n",
    "\n",
    "class LearnerType(Enum):\n",
    "    CLASSIFICATION = 1\n",
    "    REGRESSION = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, sqrt\n",
    "from xgboost import XGBClassifier, XGBRFRegressor\n",
    "\n",
    "\n",
    "# TODO: figure out parameters, especially objective \n",
    "def get_XGBModel(X, y, base_learner, task):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    if task == LearnerType.CLASSIFICATION:\n",
    "        XGBModel = XGBClassifier\n",
    "        objective = \"binary:logistic\"\n",
    "    elif task == LearnerType.REGRESSION:\n",
    "        XGBModel = XGBRFRegressor\n",
    "        objective = \"binary:logistic\"\n",
    "    else:\n",
    "        assert False, f\"Wrong arguent: {task}.\"\n",
    "\n",
    "    if base_learner == \"RF\":\n",
    "        return XGBModel(\n",
    "            learning_rate = 0.2, # 0.0 - 1.0, log, not sure if each forest can have its own, it might lead to overfitting anyway\n",
    "            max_depth = 20, # 2 - 50, 6 - 20 is another more conservative option\n",
    "            subsample = 0.63, # 0.0 - 1.0\n",
    "            # u can set only one of the colsample_by*\n",
    "            # colsample_bytree: Optional[float] = None,\n",
    "            # colsample_bylevel: Optional[float] = None,\n",
    "            colsample_bynode = floor(sqrt(m))/m, # m..num_of_features, 0 - m, log?\n",
    "            n_estimators = 10, # 100 - 500 = number of random forests in booster\n",
    "            num_parallel_tree = 10, # 100 - 500 = number of trees in each random forest\n",
    "            reg_lambda = 0, # -10. - 10.0, log = prunning of trees, higher value -> more prunning, not sure if negative values do anything\n",
    "            min_child_weight = 2, # 0.0 - 10.0, log, higher value -> less options to choose from when selecting new nodes in trees\n",
    "            objective = objective, # list at https://xgboost.readthedocs.io/en/stable/parameter.html, search for objective\n",
    "            seed=SEED, \n",
    "            seed_per_iteration=True,\n",
    "        )\n",
    "    elif base_learner == \"DecisionTree\":\n",
    "        return XGBModel(\n",
    "            objective=objective,\n",
    "            seed=SEED,\n",
    "            seed_per_iteration=True\n",
    "        )\n",
    "    else:\n",
    "        assert False, f\"Wrong arguent: {base_learner}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-sklearn settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autosklearn.pipeline.components.base import AutoSklearnPreprocessingAlgorithm\n",
    "from autosklearn.pipeline.constants import SPARSE, DENSE, UNSIGNED_DATA, INPUT\n",
    "from ConfigSpace.configuration_space import ConfigurationSpace\n",
    "\n",
    "\n",
    "class NoPreprocessing(AutoSklearnPreprocessingAlgorithm):\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, val in kwargs.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def get_properties(dataset_properties=None):\n",
    "        return {\n",
    "            \"shortname\": \"NoPreprocessing\",\n",
    "            \"name\": \"NoPreprocessing\",\n",
    "            \"handles_regression\": True,\n",
    "            \"handles_classification\": True,\n",
    "            \"handles_multiclass\": True,\n",
    "            \"handles_multilabel\": True,\n",
    "            \"handles_multioutput\": True,\n",
    "            \"is_deterministic\": True,\n",
    "            \"input\": (SPARSE, DENSE, UNSIGNED_DATA),\n",
    "            \"output\": (INPUT,),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hyperparameter_search_space(feat_type=None, dataset_properties=None):\n",
    "        return ConfigurationSpace()\n",
    "\n",
    "\n",
    "import autosklearn.pipeline.components.data_preprocessing\n",
    "autosklearn.pipeline.components.data_preprocessing.add_preprocessor(NoPreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosklearn.pipeline.components.base import AutoSklearnClassificationAlgorithm\n",
    "from autosklearn.pipeline.constants import SPARSE, DENSE, UNSIGNED_DATA, INPUT\n",
    "from ConfigSpace.configuration_space import ConfigurationSpace\n",
    "\n",
    "\n",
    "class XGBClassifier_(AutoSklearnClassificationAlgorithm):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.estimator = None\n",
    "        for key, val in kwargs.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.estimator = get_XGBModel(X, y, BASE_LEARNER, LearnerType.CLASSIFICATION).fit(X, y, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.estimator is None:\n",
    "            raise NotImplementedError()\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.estimator is None:\n",
    "            raise NotImplementedError()\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_properties(dataset_properties=None):\n",
    "        return {\n",
    "            \"shortname\": \"xgboost\",\n",
    "            \"name\": \"xgboost\",\n",
    "            \"handles_regression\": False,\n",
    "            \"handles_classification\": True,\n",
    "            \"handles_multiclass\": True,\n",
    "            \"handles_multilabel\": True,\n",
    "            \"handles_multioutput\": True,\n",
    "            \"is_deterministic\": True,\n",
    "            \"input\": (SPARSE, DENSE, UNSIGNED_DATA),\n",
    "            \"output\": (INPUT,),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hyperparameter_search_space(feat_type=None, dataset_properties=None):\n",
    "        return ConfigurationSpace()  # TODO: optimize some of the parameters\n",
    "\n",
    "import autosklearn.pipeline.components.classification\n",
    "autosklearn.pipeline.components.classification.add_classifier(XGBClassifier_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.metrics import make_scorer, roc_auc, f1\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from os.path import exists\n",
    "import shutil\n",
    "\n",
    "\n",
    "def minority_precision_(y_test, y_pred):\n",
    "    from sklearn.metrics import precision_score\n",
    "    return precision_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0)\n",
    "\n",
    "\n",
    "minority_precision = make_scorer(\n",
    "    name=\"minority_precision\",\n",
    "    score_func=minority_precision_,\n",
    "    optimum=1,\n",
    "    greater_is_better=True,\n",
    "    needs_proba=False,\n",
    "    needs_threshold=False,\n",
    ")\n",
    "\n",
    "\n",
    "TEMP = \"temp_folder\"\n",
    "\n",
    "\n",
    "# TODO: figure out parameters\n",
    "def get_AutoSklearnClassifier(X, y, task):\n",
    "    if task == Task.DATA_IMBALANCE:\n",
    "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        can_apply_smote = len(numerical_features) == X.shape[1]\n",
    "        metric=[roc_auc, minority_precision]\n",
    "        include={\n",
    "            \"data_preprocessor\": [\"NoPreprocessing\"],\n",
    "            \"balancing\": [\"none\", \"weighting\"] + ([\"SVMSMOTE\"] if can_apply_smote else []),\n",
    "            \"feature_preprocessor\": [\"no_preprocessing\"],\n",
    "            \"classifier\": [\"XGBClassifier_\"]\n",
    "        }\n",
    "    elif task == Task.INADEQUATE_FEATURES:\n",
    "        metric=[f1, minority_precision]\n",
    "        include={\n",
    "            \"data_preprocessor\": [\"NoPreprocessing\"],\n",
    "            \"balancing\": [\"none\"],\n",
    "            \"classifier\": [\"XGBClassifier_\"]\n",
    "        }\n",
    "    else:\n",
    "        assert False, f\"Wrong arguent: {task}.\"\n",
    "\n",
    "    if exists(TEMP):\n",
    "        shutil.rmtree(TEMP, ignore_errors=True)\n",
    "    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "    return AutoSklearnClassifier(\n",
    "        time_left_for_this_task=5*60,\n",
    "        metric=metric,\n",
    "        initial_configurations_via_metalearning=0,\n",
    "        ensemble_class=None,\n",
    "        include=include,\n",
    "        resampling_strategy=skf,\n",
    "        seed=SEED,\n",
    "        tmp_folder=TEMP,\n",
    "        delete_tmp_folder_after_terminate=False,\n",
    "        n_jobs=1,\n",
    "        memory_limit=3000,\n",
    "        per_run_time_limit=5*60\n",
    "    ).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103 true 8777 false\n",
      "Counter({False: 8777, True: 2103})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['JM1']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# TODO: find all of the missing datasets\n",
    "IDS = [\n",
    "    # (\"breast-w\", 15),\n",
    "    # (\"credit-approval\", 29),\n",
    "    # (\"credit-g\", 31),\n",
    "    # (\"diabetes\", 37),\n",
    "    # (\"sick\", 38),\n",
    "    # (\"spambase\", 44),\n",
    "    # (\"tic-tac-toe\", 50),\n",
    "    # (\"electricity\", 151),\n",
    "    # (\"vowel\", 307),\n",
    "    # (\"pc4\", 1049),\n",
    "    # (\"pc3\", 1050),\n",
    "    (\"JM1\", 1053),\n",
    "    # (\"KC2\", 1063),\n",
    "    # (\"kc1\", 1067),\n",
    "    # (\"pc1\", 1068),\n",
    "    # (\"bank-marketing\", 1461),\n",
    "    # (\"blood-transfusion-service-center\", 1464),\n",
    "    # (\"ilpd\", 1480),\n",
    "    # (\"madelon\", 1485),\n",
    "    # (\"nomao\", 1486),\n",
    "    # (\"ozone-level-8hr\", 1487),\n",
    "    # (\"phoneme\", 1489),\n",
    "    # (\"qsar-biodeg\", 1494),\n",
    "    # (\"adult\", 1590),\n",
    "    # (\"Bioresponse\", 4134),\n",
    "    # (\"cylinder-bands\", 6332),\n",
    "    # (\"dresses-sales\", 23381),\n",
    "    # (\"numerai28.6\", 23517),\n",
    "    # (\"churn\", 40701),\n",
    "    # (\"wilt\", 40983),\n",
    "    # (\"climate-model-simulation-crashes\", 40994),\n",
    "]\n",
    "\n",
    "\n",
    "DATASETS = []\n",
    "for dataset_name, dataset_id in IDS:\n",
    "    data = fetch_openml(data_id=dataset_id, parser=\"auto\", as_frame=True)\n",
    "\n",
    "    if data.frame.shape[0] < 1000:\n",
    "        continue\n",
    "    if len(data.target_names) != 1:\n",
    "        continue\n",
    "    target = data.target_names[0]\n",
    "    if len(data.frame[target].unique()) != 2:\n",
    "        continue\n",
    "\n",
    "    if data.frame.isna().any().any():\n",
    "        nan_percent = data.frame.isna().sum().sum() / data.frame.shape[0]\n",
    "        assert nan_percent < 0.01, nan_percent\n",
    "        data.frame.dropna(inplace=True)\n",
    "\n",
    "    X = data.frame.drop(columns=[target])\n",
    "    (l1, l2), (c1, c2) = np.unique(data.frame[target], return_counts=True)\n",
    "    (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
    "    # NOTE: convention that minority label will always be 1\n",
    "    y = data.frame[target] == l1\n",
    "    DATASETS.append((dataset_name, X, y))\n",
    "\n",
    "[name for name, _, _ in DATASETS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score, \n",
    "    fbeta_score, \n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: which metric and how to compute for which task?\n",
    "# TODO: measure train time and inference time for each fold\n",
    "def validate(X, y, model, task):\n",
    "    res = {}\n",
    "    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        if hasattr(model, \"refit\"):\n",
    "            model = model.refit(X_train, y_train)\n",
    "        else:\n",
    "            # NOTE:  XGB docs says that calling fit() multiple times\n",
    "            # will cause re-fit, unless xgb_model parameter\n",
    "            # is provided explicitly\n",
    "            model = model.fit(X_train, y_train)\n",
    "\n",
    "        if task == Task.DATA_IMBALANCE:\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "            y_pred = thresholds[(tpr - fpr).argmax()] < y_prob\n",
    "\n",
    "            res[fold] = {\n",
    "                \"auc_roc\": roc_auc_score(y_test, y_prob),\n",
    "                \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "\n",
    "                \"minority_precision\": precision_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
    "                \"majority_precision\": precision_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
    "                \"weighted_precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "                \"macro_precision\": precision_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "                \n",
    "                \"minority_recall\": recall_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
    "                \"majority_recall\": recall_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
    "                \"weighted_recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "                \"macro_recall\": recall_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "                \n",
    "                \"minority_f1\": f1_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
    "                \"majority_f1\": f1_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
    "                \"weighted_f1\": f1_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "                \"macro_f1\": f1_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "                \n",
    "                \"minority_f2\": fbeta_score(y_test, y_pred, beta=2, pos_label=1, average='binary', zero_division=0.0),\n",
    "                \"majority_f2\": fbeta_score(y_test, y_pred, beta=2, pos_label=0, average='binary', zero_division=0.0),\n",
    "                \"weighted_f2\": fbeta_score(y_test, y_pred, beta=2, average='weighted', zero_division=0.0),\n",
    "                \"macro_f2\": fbeta_score(y_test, y_pred, beta=2, average='macro', zero_division=0.0),\n",
    "                \n",
    "                \"y_pred\": [float(y) for y in y_pred],\n",
    "                \"y_prob\": [float(y) for y in y_prob],\n",
    "                \"y_test\": [float(y) for y in y_test],\n",
    "            }\n",
    "        else:\n",
    "            pass  # TODO\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JM1', 0.2),\n",
       " ('JM1', 0.19),\n",
       " ('JM1', 0.18),\n",
       " ('JM1', 0.17),\n",
       " ('JM1', 0.16),\n",
       " ('JM1', 0.15),\n",
       " ('JM1', 0.14),\n",
       " ('JM1', 0.13),\n",
       " ('JM1', 0.12),\n",
       " ('JM1', 0.11),\n",
       " ('JM1', 0.1),\n",
       " ('JM1', 0.09),\n",
       " ('JM1', 0.08),\n",
       " ('JM1', 0.07),\n",
       " ('JM1', 0.06),\n",
       " ('JM1', 0.05),\n",
       " ('JM1', 0.04),\n",
       " ('JM1', 0.03),\n",
       " ('JM1', 0.02),\n",
       " ('JM1', 0.01)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "\n",
    "IMBALANCED_DATSETS = []\n",
    "ratios = [0.5, 0.25] + [r/100 for r in range(1, 21)]\n",
    "for name, X, y in DATASETS:\n",
    "    for ratio in sorted(ratios, reverse=True):\n",
    "        (l1, l2), (c1, c2) = np.unique(data.frame[target], return_counts=True)\n",
    "        (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
    "        new_minority_count = int(c2 * ratio)\n",
    "        if c1 < new_minority_count:\n",
    "            continue\n",
    "        X, y = make_imbalance(X, y, sampling_strategy={0: c2, 1: new_minority_count}, random_state=SEED)\n",
    "        IMBALANCED_DATSETS.append((name, ratio, X, y))\n",
    "\n",
    "[(name, ratio) for name, ratio, _, _ in IMBALANCED_DATSETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_scores = {}\n",
    "for name, ratio, X, y in IMBALANCED_DATSETS:\n",
    "    model = get_AutoSklearnClassifier(X, y, Task.DATA_IMBALANCE)\n",
    "    imbalance_scores[f\"{name}(ratio={ratio:.2f})\"] = validate(X, y, model, Task.DATA_IMBALANCE)\n",
    "\n",
    "imbalance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOISY_DATSETS = []\n",
    "noise_amount = [a/100 for a in range(1, 11)]\n",
    "for name, X, y in DATASETS:\n",
    "    for noise in sorted(noise_amount):\n",
    "        # TODO: add 1% of noise to X each iteration, dont over write previous noise\n",
    "        # X = ...\n",
    "        pass\n",
    "\n",
    "[(name, noise) for name, noise, _, _ in NOISY_DATSETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NoisyXGBClassifier:\n",
    "    def fit(self, X, y):\n",
    "        X_train, y_train = self.remove_noise(X, y)\n",
    "        model = get_XGBModel(X_train, y_train, BASE_LEARNER, LearnerType.CLASSIFICATION)\n",
    "        model = model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def remove_noise(self, X, y):\n",
    "        model = get_XGBModel(X, y, BASE_LEARNER, LearnerType.REGRESSION)\n",
    "        model = model.fit(X, y)\n",
    "        # TODO: create a new dataset instead of renaming labels\n",
    "        X_transformed, y_transformed = X, model.predict(X)\n",
    "        return X_transformed, y_transformed\n",
    "\n",
    "\n",
    "noisy_scores = {}\n",
    "for name, noise, X, y in NOISY_DATSETS:\n",
    "    model = NoisyXGBClassifier()\n",
    "    noisy_scores[f\"{name}(noise={noise})\"] = validate(X, y, model, Task.NOISY_DATA)\n",
    "\n",
    "noisy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JM1', 0.01),\n",
       " ('JM1', 0.02),\n",
       " ('JM1', 0.03),\n",
       " ('JM1', 0.04),\n",
       " ('JM1', 0.05),\n",
       " ('JM1', 0.06),\n",
       " ('JM1', 0.07),\n",
       " ('JM1', 0.08),\n",
       " ('JM1', 0.09),\n",
       " ('JM1', 0.1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_DATSETS = []\n",
    "hidden_amount = [a/100 for a in range(1, 11)]\n",
    "for name, X, y in DATASETS:\n",
    "    for hidden in sorted(hidden_amount):\n",
    "        # TODO: create hidden instances\n",
    "        X, X_hidden = X, X\n",
    "        HIDDEN_DATSETS.append((name, hidden, X, X_hidden, y))\n",
    "\n",
    "[(name, hidden) for name, hidden, _, _, _ in HIDDEN_DATSETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# TODO: while there are any hidden instances\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     model \u001b[38;5;241m=\u001b[39m BaggingClassifier(\n\u001b[1;32m     12\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mget_XGBModel(X, y, BASE_LEARNER, LearnerType\u001b[38;5;241m.\u001b[39mCLASSIFICATION),\n\u001b[1;32m     13\u001b[0m         n_estimators \u001b[38;5;241m=\u001b[39m N_ESTIMATORS_BAGGING,\n\u001b[1;32m     14\u001b[0m         n_jobs \u001b[38;5;241m=\u001b[39m N_JOBS_BAGGING,\n\u001b[1;32m     15\u001b[0m         random_state \u001b[38;5;241m=\u001b[39m SEED\n\u001b[1;32m     16\u001b[0m     )\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     y_hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_hidden)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# TODO: select instances from X_hidden that most of the ensemble agrees on,\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# add them together with labels to X, y and remove from X_hidden\u001b[39;00m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/sklearn/ensemble/_bagging.py:338\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[1;32m    330\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    331\u001b[0m     X,\n\u001b[1;32m    332\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m )\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/sklearn/ensemble/_bagging.py:473\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    470\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[0;32m--> 473\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    492\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/sklearn/ensemble/_bagging.py:141\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[1;32m    138\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    140\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[indices][:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X[indices]\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/xgboost/sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1491\u001b[0m (\n\u001b[1;32m   1492\u001b[0m     model,\n\u001b[1;32m   1493\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1499\u001b[0m )\n\u001b[1;32m   1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1501\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1502\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1517\u001b[0m )\n\u001b[0;32m-> 1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/PV056/project/venv/lib/python3.10/site-packages/xgboost/core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[0;32m-> 2051\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m     )\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# TODO: how to set up bagging\n",
    "N_ESTIMATORS_BAGGING = 10\n",
    "N_JOBS_BAGGING = 1\n",
    "\n",
    "semi_scores = {}\n",
    "for name, hidden, X, X_hidden, y in HIDDEN_DATSETS:\n",
    "    while True: # TODO: while there are any hidden instances\n",
    "        model = BaggingClassifier(\n",
    "            estimator=get_XGBModel(X, y, BASE_LEARNER, LearnerType.CLASSIFICATION),\n",
    "            n_estimators = N_ESTIMATORS_BAGGING,\n",
    "            n_jobs = N_JOBS_BAGGING,\n",
    "            random_state = SEED\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        y_hidden = model.predict_proba(X_hidden)\n",
    "        # TODO: select instances from X_hidden that most of the ensemble agrees on,\n",
    "        # add them together with labels to X, y and remove from X_hidden\n",
    "    model = get_XGBModel(X, y, BASE_LEARNER, LearnerType.CLASSIFICATION)\n",
    "    semi_scores[f\"{name}(noise={noise})\"] = validate(X, y, model, Task.SEMI_SUPERVISED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inadequate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inadequate_features_scores = {}\n",
    "for name, X, y in DATASETS:\n",
    "    model = get_AutoSklearnClassifier(X, y, task=Task.INADEQUATE_FEATURES)\n",
    "    inadequate_features_scores[name] = validate(X, y, model, Task.FEATURE_INADEQUACY)\n",
    "\n",
    "inadequate_features_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
