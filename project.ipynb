{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-sklearn settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "TIME = 15\n",
    "TIME_PER_RUN = None\n",
    "META = 0\n",
    "TEMP = \"temp_folder\"\n",
    "K_FOLDS = 5\n",
    "N_JOBS = None\n",
    "MEM = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoostClassifier settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, sqrt\n",
    "LEARNING_RATE = 0.2, # 0.0 - 1.0, log, not sure if each forest can have its own, it might lead to overfitting anyway\n",
    "\n",
    "MAX_DEPTH = 20, # 2 - 50, 6 - 20 is another more conservative option\n",
    "SUBSAMPLE = 0.63, # 0.0 - 1.0\n",
    "# u can set only one of the colsample_by*\n",
    "# colsample_bytree: Optional[float] = None,\n",
    "# colsample_bylevel: Optional[float] = None,\n",
    "COLSAMPLE_BYNODE = lambda m: floor(sqrt(m))/m, # m..num_of_features, 0 - m, log?\n",
    "N_ESTIMATORS = 10, # 100 - 500 = number of random forests in booster\n",
    "NUM_PARALLEL_TREE = 10, # 100 - 500 = number of trees in each random forest\n",
    "REG_LAMBDA = 0, # -10. - 10.0, log = prunning of trees, higher value -> more prunning, not sure if negative values do anything\n",
    "MIN_CHILD_WEIGHT = 2, # 0.0 - 10.0, log, higher value -> less options to choose from when selecting new nodes in trees\n",
    "\n",
    "OBJECTIVE = 'binary:logistic', # list at https://xgboost.readthedocs.io/en/stable/parameter.html, search for objective\n",
    "SEED_PER_ITERATION=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoostRegressor settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, sqrt\n",
    "LEARNING_RATE_REGRESSION = 0.2, # 0.0 - 1.0, log, not sure if each forest can have its own, it might lead to overfitting anyway\n",
    "\n",
    "MAX_DEPTH_REGRESSION = 20, # 2 - 50, 6 - 20 is another more conservative option\n",
    "SUBSAMPLE_REGRESSION = 0.63, # 0.0 - 1.0\n",
    "# u can set only one of the colsample_by*\n",
    "# colsample_bytree: Optional[float] = None,\n",
    "# colsample_bylevel: Optional[float] = None,\n",
    "COLSAMPLE_BYNODE_REGRESSION = lambda m: floor(sqrt(m))/m, # m..num_of_features, 0 - m, log?\n",
    "N_ESTIMATORS_REGRESSION = 10, # 100 - 500 = number of random forests in booster\n",
    "NUM_PARALLEL_TREE_REGRESSION = 10, # 100 - 500 = number of trees in each random forest\n",
    "REG_LAMBDA_REGRESSION = 0, # -10. - 10.0, log = prunning of trees, higher value -> more prunning, not sure if negative values do anything\n",
    "MIN_CHILD_WEIGHT_REGRESSION = 2, # 0.0 - 10.0, log, higher value -> less options to choose from when selecting new nodes in trees\n",
    "\n",
    "OBJECTIVE_REGRESSION = 'binary:logistic', # list at https://xgboost.readthedocs.io/en/stable/parameter.html, search for objective\n",
    "SEED_PER_ITERATION_REGRESSION=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# TODO: find all of the missing datasets\n",
    "IDS = [\n",
    "    # (\"breast-w\", 15),\n",
    "    # (\"credit-approval\", 29),\n",
    "    # (\"credit-g\", 31),\n",
    "    # (\"diabetes\", 37),\n",
    "    # (\"sick\", 38),\n",
    "    # (\"spambase\", 44),\n",
    "    # (\"tic-tac-toe\", 50),\n",
    "    # (\"electricity\", 151),\n",
    "    # (\"vowel\", 307),\n",
    "    # (\"pc4\", 1049),\n",
    "    # (\"pc3\", 1050),\n",
    "    (\"JM1\", 1053),\n",
    "    # (\"KC2\", 1063),\n",
    "    # (\"kc1\", 1067),\n",
    "    # (\"pc1\", 1068),\n",
    "    # (\"bank-marketing\", 1461),\n",
    "    # (\"blood-transfusion-service-center\", 1464),\n",
    "    # (\"ilpd\", 1480),\n",
    "    # (\"madelon\", 1485),\n",
    "    # (\"nomao\", 1486),\n",
    "    # (\"ozone-level-8hr\", 1487),\n",
    "    # (\"phoneme\", 1489),\n",
    "    # (\"qsar-biodeg\", 1494),\n",
    "    # (\"adult\", 1590),\n",
    "    # (\"Bioresponse\", 4134),\n",
    "    # (\"cylinder-bands\", 6332),\n",
    "    # (\"dresses-sales\", 23381),\n",
    "    # (\"numerai28.6\", 23517),\n",
    "    # (\"churn\", 40701),\n",
    "    # (\"wilt\", 40983),\n",
    "    # (\"climate-model-simulation-crashes\", 40994),\n",
    "]\n",
    "\n",
    "\n",
    "DATASETS = []\n",
    "for dataset_name, dataset_id in IDS:\n",
    "    data = fetch_openml(data_id=dataset_id, parser=\"auto\", as_frame=True)\n",
    "\n",
    "    if data.frame.shape[0] < 1000:\n",
    "        continue\n",
    "    if len(data.target_names) != 1:\n",
    "        continue\n",
    "    target = data.target_names[0]\n",
    "    if len(data.frame[target].unique()) != 2:\n",
    "        continue\n",
    "\n",
    "    X = data.frame.drop(columns=[target])\n",
    "    (l1, l2), (c1, c2) = np.unique(data.frame[target], return_counts=True)\n",
    "    (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
    "    y = data.frame[target] == l1\n",
    "    DATASETS.append((dataset_name, X, y))\n",
    "\n",
    "[name for name, _, _ in DATASETS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score, \n",
    "    fbeta_score, \n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "def validate(y_test, y_prob):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    y_pred = thresholds[(tpr - fpr).argmax()] < y_prob\n",
    "    return {\n",
    "        \"auc_roc\": roc_auc_score(y_test, y_prob),\n",
    "        \"weighted_precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "        \"macro_precision\": precision_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "        \"weighted_recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "        \"macro_recall\": recall_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "        \"weighted_f1\": f1_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "        \"macro_f1\": f1_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
    "        \"weighted_f2\": fbeta_score(y_test, y_pred, beta=2, average='weighted', zero_division=0.0),\n",
    "        \"macro_f2\": fbeta_score(y_test, y_pred, beta=2, average='macro', zero_division=0.0),\n",
    "        \"y_pred\": [float(y) for y in y_pred],\n",
    "        \"y_prob\": [float(y) for y in y_prob],\n",
    "        \"y_test\": [float(y) for y in y_test],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "\n",
    "IMBALANCED_DATSETS = []\n",
    "ratios = [0.5, 0.25] + [r/100 for r in range(1, 21)]\n",
    "for name, X, y in DATASETS:\n",
    "    for ratio in sorted(ratios, reverse=True):\n",
    "        try:\n",
    "            (l1, l2), (c1, c2) = np.unique(data.frame[target], return_counts=True)\n",
    "            (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
    "            X, y = make_imbalance(X, y, sampling_strategy={0: c2, 1: int(c2 * ratio)}, random_state=SEED)\n",
    "            IMBALANCED_DATSETS.append((name, ratio, X, y))\n",
    "        except ValueError as e:\n",
    "            if \"With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples.\" in str(e):\n",
    "                continue\n",
    "            raise e\n",
    "[(name, ratio) for name, ratio, _, _ in IMBALANCED_DATSETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of auto-sklearn version that supports SMOTE, source has to be downloaded localy\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.insert(0, \"../my_autosklearn\")\n",
    "    from autosklearn.classification import AutoSklearnClassifier\n",
    "    from autosklearn.metrics import roc_auc, recall_weighted\n",
    "\n",
    "    import autosklearn.pipeline.components.data_preprocessing\n",
    "    from no_data_preprocessor import NoPreprocessing\n",
    "    autosklearn.pipeline.components.data_preprocessing.add_preprocessor(NoPreprocessing)\n",
    "\n",
    "\n",
    "from os.path import exists\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "imbalance_scores = {}\n",
    "for name, ratio, X, y in IMBALANCED_DATSETS:\n",
    "\n",
    "    if exists(TEMP):\n",
    "        shutil.rmtree(TEMP, ignore_errors=True)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "    model = AutoSklearnClassifier(\n",
    "        time_left_for_this_task=TIME,\n",
    "        metric=[roc_auc, recall_weighted],\n",
    "        initial_configurations_via_metalearning=META,\n",
    "        include={\n",
    "            \"data_preprocessor\": [\"no_preprocessing\"],\n",
    "            \"balancing\": [\"none\", \"weighting\", \"SVMSMOTE\"],\n",
    "            \"feature_preprocessor\": [\"no_preprocessing\"],\n",
    "            \"classifier\": [\"xgboost\"]\n",
    "            },\n",
    "        resampling_strategy=skf,\n",
    "        seed=SEED,\n",
    "        tmp_folder=TEMP,\n",
    "        delete_tmp_folder_after_terminate=True,\n",
    "        n_jobs=N_JOBS,\n",
    "        memory_limit=MEM,\n",
    "        per_run_time_limit=TIME_PER_RUN\n",
    "    ).fit(X, y)\n",
    "\n",
    "    imbalance_scores[f\"{name}(ratio={ratio:.2f})\"] = {}\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model = model.refit(X_train, y_train)\n",
    "        imbalance_scores[f\"{name}(ratio={ratio:.2f})\"][fold] = validate(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "imbalance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISY_DATSETS = []\n",
    "noise_amount = [a/100 for a in range(1, 11)]\n",
    "for name, X, y in DATASETS:\n",
    "    for noise in sorted(noise_amount):\n",
    "        # TODO: add 1% of noise to X each iteration, dont over write previous noise\n",
    "        # X = ...\n",
    "        pass\n",
    "\n",
    "[(name, noise) for name, noise, _, _ in NOISY_DATSETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "\n",
    "def transform(X, y):\n",
    "    model = XGBRegressor(\n",
    "        learning_rate = LEARNING_RATE_REGRESSION,\n",
    "        max_depth = MAX_DEPTH_REGRESSION,\n",
    "        subsample = SUBSAMPLE_REGRESSION,\n",
    "        colsample_bynode = COLSAMPLE_BYNODE_REGRESSION(m),\n",
    "        n_estimators = N_ESTIMATORS_REGRESSION,\n",
    "        num_parallel_tree = NUM_PARALLEL_TREE_REGRESSION,\n",
    "        reg_lambda = REG_LAMBDA_REGRESSION,\n",
    "        min_child_weight = MIN_CHILD_WEIGHT_REGRESSION,\n",
    "        objective = OBJECTIVE_REGRESSION,\n",
    "        seed=SEED, \n",
    "        seed_per_iteration=SEED_PER_ITERATION_REGRESSION,\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    # TODO: create a new dataset instead of renaming labels\n",
    "    X_transformed, y_transformed = X, model.predict(X)\n",
    "    return X_transformed, y_transformed\n",
    "\n",
    "\n",
    "noisy_scores = {}\n",
    "for name, noise, X, y in NOISY_DATSETS:\n",
    "    m = len(X[0])\n",
    "    noisy_scores[f\"{name}(noise={noise})\"] = {}\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, y_train = transform(X_train, y_train)\n",
    "        model = XGBClassifier(\n",
    "            learning_rate = LEARNING_RATE,\n",
    "            max_depth = MAX_DEPTH,\n",
    "            subsample = SUBSAMPLE,\n",
    "            colsample_bynode = COLSAMPLE_BYNODE(m),\n",
    "            n_estimators = N_ESTIMATORS,\n",
    "            num_parallel_tree = NUM_PARALLEL_TREE,\n",
    "            reg_lambda = REG_LAMBDA,\n",
    "            min_child_weight = MIN_CHILD_WEIGHT,\n",
    "            objective = OBJECTIVE,\n",
    "            seed=SEED, \n",
    "            seed_per_iteration=SEED_PER_ITERATION,\n",
    "        ).fit(X_train, y_train)\n",
    "        noisy_scores[f\"{name}(noise={noise})\"][fold] = validate(y_test, model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DATSETS = []\n",
    "hidden_amount = [a/100 for a in range(1, 11)]\n",
    "for name, X, y in DATASETS:\n",
    "    for hidden in sorted(noise_amount):\n",
    "        # TODO: create hidden instances\n",
    "        X, X_hidden = X, X\n",
    "        HIDDEN_DATSETS.append((name, hidden, X, X_hidden, y))\n",
    "\n",
    "[(name, hidden) for name, hidden, _, _, _ in HIDDEN_DATSETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "for name, hidden, X, X_hidden, y in HIDDEN_DATSETS:\n",
    "    while True: # TODO: while there are any hidden instances\n",
    "        model = BaggingClassifier(\n",
    "            estimator=XGBClassifier(\n",
    "                learning_rate = LEARNING_RATE,\n",
    "                max_depth = MAX_DEPTH,\n",
    "                subsample = SUBSAMPLE,\n",
    "                colsample_bynode = COLSAMPLE_BYNODE(m),\n",
    "                n_estimators = N_ESTIMATORS,\n",
    "                num_parallel_tree = NUM_PARALLEL_TREE,\n",
    "                reg_lambda = REG_LAMBDA,\n",
    "                min_child_weight = MIN_CHILD_WEIGHT,\n",
    "                objective = OBJECTIVE,\n",
    "                seed=SEED, \n",
    "                seed_per_iteration=SEED_PER_ITERATION,\n",
    "            ),\n",
    "            n_estimators = 10,\n",
    "            n_jobs = N_JOBS,\n",
    "            random_state = SEED\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        y_hidden = model.predict_proba(X_hidden)\n",
    "        # TODO: select instances from X_hidden that most of the ensemble agrees on,\n",
    "        # add them together with labels to X, y and remove from X_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inadequate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of auto-sklearn version that supports SMOTE, it has to be downloaded localy\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.insert(0, \"../my_autosklearn\")\n",
    "    from autosklearn.classification import AutoSklearnClassifier\n",
    "    from autosklearn.metrics import roc_auc, recall_weighted\n",
    "\n",
    "    import autosklearn.pipeline.components.data_preprocessing\n",
    "    from no_data_preprocessor import NoPreprocessing\n",
    "    autosklearn.pipeline.components.data_preprocessing.add_preprocessor(NoPreprocessing)\n",
    "\n",
    "\n",
    "from os.path import exists\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "feature_preprocessor = [\n",
    "    'densifier', 'extra_trees_preproc_for_classification', 'fast_ica', 'feature_agglomeration', \n",
    "    'kernel_pca', 'kitchen_sinks', 'liblinear_svc_preprocessor', 'no_preprocessing', 'nystroem_sampler', 'pca', \n",
    "    'polynomial', 'random_trees_embedding', 'select_percentile_classification', 'select_rates_classification', \n",
    "    'truncatedSVD'\n",
    "]\n",
    "\n",
    "\n",
    "inadequate_features_scores = {}\n",
    "for name, X, y in DATASETS:\n",
    "\n",
    "    if exists(TEMP):\n",
    "        shutil.rmtree(TEMP, ignore_errors=True)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "    model = AutoSklearnClassifier(\n",
    "        time_left_for_this_task=TIME,\n",
    "        metric=[roc_auc, recall_weighted],\n",
    "        initial_configurations_via_metalearning=META,\n",
    "        include={\n",
    "            \"data_preprocessor\": [\"no_preprocessing\"],\n",
    "            \"balancing\": [\"none\"],\n",
    "            \"feature_preprocessor\": feature_preprocessor,\n",
    "            \"classifier\": [\"xgboost\"]\n",
    "            },\n",
    "        resampling_strategy=skf,\n",
    "        seed=SEED,\n",
    "        tmp_folder=TEMP,\n",
    "        delete_tmp_folder_after_terminate=True,\n",
    "        n_jobs=N_JOBS,\n",
    "        memory_limit=MEM,\n",
    "        per_run_time_limit=TIME_PER_RUN\n",
    "    ).fit(X, y)\n",
    "\n",
    "    inadequate_features_scores[name] = {}\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model = model.refit(X_train, y_train)\n",
    "        inadequate_features_scores[name][fold] = validate(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "inadequate_features_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
