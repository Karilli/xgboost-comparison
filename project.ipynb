{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PV056 project**\n",
        "\n",
        "**Theme:** XGBoost with random forest base learner\n",
        "\n",
        "**Description:**\n",
        "Use XGBoost with the random forest as a base learner.\n",
        "\n",
        "**Supervisors:**\n",
        "- Bc. Terézia Mikulová, učo [483657](https://is.muni.cz/auth/person/483657) (supervisor)\n",
        "- doc. RNDr. Lubomír Popelínský, Ph.D., učo [1945](https://is.muni.cz/auth/person/1945) (consultant)\n",
        "\n",
        "\n",
        "**Students:**\n",
        "- Josef Karas, učo [511737](https://is.muni.cz/auth/person/511737)\n",
        "- Filip Chladek, učo [514298](https://is.muni.cz/auth/person/514298)\n",
        "- Bc. Martin Beňa, učo [485152](https://is.muni.cz/auth/person/485152)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JQRJrPGsBpgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **First, copy this notebook to your drive.**\n",
        "\n",
        "To run this notebook, you need the modified auto-sklearn library and other modules provided in the link below. These three cells will download, unpack, install requests libraries, and clean excessive files that are not needed.\n",
        "\\\n",
        "**Run these cells only once OR if you wish to start again.**"
      ],
      "metadata": {
        "id": "XSuOkNn83l6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!gdown --fuzzy https://drive.google.com/file/d/17zt3tN_xUB_GDHbtzmRG78Ds3Vd9vugX/view?usp=sharing"
      ],
      "metadata": {
        "id": "JuZl8RdxUqf3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "shutil.unpack_archive('/content/project_tools.zip', '/content', 'zip')\n",
        "os.remove('/content/project_tools.zip')"
      ],
      "metadata": {
        "id": "9hpKy3yZIJEv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r /content/requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3hkSORRBJdzw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing the pip requirements, **you need to restart the relation to apply changes made by installed libraries.**\n",
        "\n",
        "You can restart relation with GUI:\n",
        "\\\n",
        "**Runtime** -> **Restart relation**"
      ],
      "metadata": {
        "id": "7ZlEYtQTj_xD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2HaZNVRH-d8"
      },
      "source": [
        "# Common settings\n",
        "\n",
        "Core functions are implemented and imported from modul ***utils.py***.\n",
        "\n",
        "*   Params.BASE_LEARNER - is used to chose base learner from 2 variants (Random forest and Decision tree).\n",
        "*   MEM - Miximum amout of allocated memory.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aditional info\n",
        "The outputs of trained models and their results are stored in the results folder in **.pkl** format.\n",
        "\n",
        "In the notebook, it is mentioned that the ```n_jobs``` parameter is set to -1 everywhere. However, if users prefer to train their own Auto-sklearn and Bagging models instead of using the provided **.pkl** files, they should set the ```n_jobs``` parameter for XGBoost to 1. Failing to do so may result in the spawning of too many threads, which can negatively impact performance.\n"
      ],
      "metadata": {
        "id": "Tm6WRVHy3izh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "gEDjy4LIH-eA"
      },
      "outputs": [],
      "source": [
        "from utils import (\n",
        "    Task, LearnerType, Params,\n",
        "    get_AutoSklearnClassifier, get_XGBModel,\n",
        "    AutoSklearnClassifier, XGBClassifier,\n",
        "    dump_pkl, dump_json, load_pkl, load_json,\n",
        "    sorted_class_count\n",
        ")\n",
        "import inspect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piHqqoVLH-eB"
      },
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "nlCKmFBrH-eC"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(Params.SEED)\n",
        "\n",
        "IDS = [\n",
        "    (\"credit-g\", 31),\n",
        "    (\"spambase\", 44),\n",
        "    (\"electricity\", 151),\n",
        "    (\"pc4\", 1049),\n",
        "    (\"pc3\", 1050),\n",
        "    (\"JM1\", 1053),\n",
        "    (\"kc1\", 1067),\n",
        "    (\"pc1\", 1068),\n",
        "    (\"bank-marketing/bank-marketing-full\", 1461),\n",
        "    (\"madelon\", 1485),\n",
        "    (\"ozone-level-8hr\", 1487),\n",
        "    (\"phoneme\", 1489),\n",
        "    (\"qsar-biodeg\", 1494),\n",
        "    (\"churn\", 40701),\n",
        "]\n",
        "\n",
        "\n",
        "DATASETS = []\n",
        "for dataset_name, dataset_id in IDS:\n",
        "    data = fetch_openml(data_id=dataset_id, parser=\"auto\", as_frame=True)\n",
        "    assert len(data.target_names) == 1\n",
        "    target = data.target_names[0]\n",
        "\n",
        "    if data.frame.shape[0] < 1000:\n",
        "        continue\n",
        "\n",
        "    if len(data.frame[target].unique()) != 2:\n",
        "        continue\n",
        "\n",
        "    X = data.frame.drop(columns=[target])\n",
        "    (l1, l2), (c1, c2) = np.unique(data.frame[target], return_counts=True)\n",
        "    (c1, l1), (c2, l2) = sorted(((c1, l1), (c2, l2)))\n",
        "    # NOTE: convention that minority label will always be 1\n",
        "    y = data.frame[target] == l1\n",
        "    DATASETS.append((dataset_name, X, y))\n",
        "\n",
        "\n",
        "[name for name, _, _ in DATASETS]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical features\n",
        "\n",
        "Categorical features are experimental and require ```enable_categorical=True``` in XGBoost. In our case, we decide to use OneHotEncoding instead to avoid using experimental features."
      ],
      "metadata": {
        "id": "aH8Z15-JiDNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GvI9Jrx1H-eC"
      },
      "outputs": [],
      "source": [
        "# OneHotEncoding\n",
        "\n",
        "for i, (name, X, y) in enumerate(DATASETS):\n",
        "    categorical_columns = X.select_dtypes(include=['category']).columns\n",
        "    if len(categorical_columns) != 0:\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown=\"error\"), categorical_columns)\n",
        "            ],\n",
        "            remainder='passthrough'  # Pass through numerical columns without any transformation\n",
        "        )\n",
        "        X = pd.DataFrame(preprocessor.fit_transform(X, y))\n",
        "        DATASETS[i] = name, X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVkIuAHaH-eD"
      },
      "source": [
        "## Datasets info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "M9NkAlU9H-eD"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "headers = [\"dataset_name\", \"IR\", \"#minority\", \"#majority\", \"#instances\", \"#features\", \"#int features\", \"#float features\", \"#category features\", \"#NaN\"]\n",
        "table = []\n",
        "for name, X, y in DATASETS:\n",
        "    c1, c2 = sorted_class_count(y)\n",
        "    int_features = len(X.select_dtypes(include=['int64']).columns)\n",
        "    float_features = len(X.select_dtypes(include=['float64']).columns)\n",
        "    category_features = len(X.select_dtypes(include=['category']).columns)\n",
        "    table.append([name, round(c1/c2, 2), c1, c2, X.shape[0], X.shape[1], int_features, float_features, category_features, X.isna().sum().sum()])\n",
        "table.sort(key=lambda i: i[1])\n",
        "\n",
        "print(tabulate(table, headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "jGu0L_ZoH-eE"
      },
      "source": [
        "## Utils for validation\n",
        "\n",
        "The following cell defines a utility function for validating machine learning models using stratified k-fold cross-validation. It supports models from AutoSklearnClassifier and XGBClassifier and evaluates them across various metrics.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**Imports:**\n",
        "Essential libraries for model validation, timing, and performance metrics.\n",
        "**Function Parameters:**\n",
        "*   *X:* Features\n",
        "*   *y:* Target\n",
        "*   *model:* Model to evaluate\n",
        "*   *task:* Task type\n",
        "*   *name:* Model name\n",
        "*   *ratio:* Data split ratio\n",
        "*   *extra:* Additional data for semi-supervised learning\n",
        "\n",
        "**Workflow:**\n",
        "1.   *Model Handling:* Extracts the core model and initializes relevant structures.\n",
        "2.   *K-Fold Split:* Performs stratified k-fold cross-validation.\n",
        "3.   *Training & Testing:* Splits data, applies preprocessing, trains the model, and makes predictions.\n",
        "4.   *Metrics Calculation:* Computes metrics like ROC AUC, confusion matrix, precision, recall, F1 score, and F-beta score.\n",
        "5.   *Results Storage:* Saves results to a JSON file for each fold.\n",
        "This ensures efficient and consistent model evaluation, capturing detailed performance metrics for analysis and comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "nYVyu3ieH-eE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import perf_counter\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    fbeta_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "def validate(X, y, model, task, name, ratio, extra=None):\n",
        "    y = pd.Series(y)\n",
        "\n",
        "    if isinstance(model, AutoSklearnClassifier):\n",
        "        model = model.get_models_with_weights()[0][1]\n",
        "        _, clf = model.steps.pop()\n",
        "    elif isinstance(model, XGBClassifier):\n",
        "        clf = model\n",
        "        class Model: pass\n",
        "        model = Model()\n",
        "        model.steps = []\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    res = load_json(Params.BASE_LEARNER, task, name, ratio)\n",
        "    skf = StratifiedKFold(n_splits=Params.K_FOLDS, shuffle=True, random_state=Params.SEED)\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        if str(fold) in res:\n",
        "            continue\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        if task == Task.SEMI_SUPERVISED:\n",
        "            X_extra_train, y_extra_train = extra\n",
        "            X_train = np.concatenate([X_train, X_extra_train])\n",
        "            y_train = np.concatenate([y_train, y_extra_train])\n",
        "\n",
        "        for _, step in model.steps:\n",
        "            if hasattr(step, \"fit_resample\"):\n",
        "                X_train, y_train = step.fit_resample(X_train, y_train)\n",
        "            elif hasattr(step, \"fit\") and hasattr(step, \"transform\"):\n",
        "                preprocesor = step.fit(X_train, y_train)\n",
        "                X_train = preprocesor.transform(X_train)\n",
        "                X_test = preprocesor.transform(X_test)\n",
        "            else:\n",
        "                assert False, f\"This step is not a transformer or resampler: {step}.\"\n",
        "\n",
        "        train_time = perf_counter()\n",
        "        clf = clf.fit(X_train, y_train)\n",
        "        train_time = perf_counter() - train_time\n",
        "\n",
        "        inference_time = perf_counter()\n",
        "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "        inference_time = perf_counter() - inference_time\n",
        "\n",
        "        if task in (Task.DATA_IMBALANCE, Task.FEATURE_INADEQUACY):\n",
        "            y_test_threshold, y_test, y_prob_threshold, y_prob = train_test_split(\n",
        "                y_test, y_prob, test_size=0.5, random_state=Params.SEED, stratify=y_test\n",
        "            )\n",
        "            fpr, tpr, thresholds = roc_curve(y_test_threshold, y_prob_threshold)\n",
        "            threshold_selection = {\n",
        "                \"(3*tpr*(1-fpr)/(2*(1-fpr)+tpr)\": thresholds[(3*tpr*(1-fpr)/(2*(1-fpr)+tpr)).argmax()] <= y_prob,\n",
        "                \"tpr-fpr\": thresholds[(tpr-fpr).argmax()] <= y_prob,\n",
        "            }\n",
        "        else:\n",
        "            threshold_selection = {\n",
        "                \"None\": clf.predict(X_test)\n",
        "            }\n",
        "\n",
        "        for threshold, y_pred in threshold_selection.items():\n",
        "            if threshold not in res:\n",
        "                res[threshold] = {}\n",
        "            res[threshold][fold] = {\n",
        "                \"cpu_count\": len(os.sched_getaffinity(0)),\n",
        "                \"train_time\": train_time,\n",
        "                \"inference_time\": inference_time,\n",
        "\n",
        "                \"auc_roc\": roc_auc_score(y_test, y_prob),\n",
        "                \"confusion_matrix\": [int(n) for n in confusion_matrix(y_test, y_pred).ravel()],\n",
        "\n",
        "                \"minority_precision\": precision_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
        "                \"majority_precision\": precision_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
        "\n",
        "                \"minority_recall\": recall_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
        "                \"majority_recall\": recall_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
        "\n",
        "                \"minority_f1\": f1_score(y_test, y_pred, pos_label=1, average='binary', zero_division=0.0),\n",
        "                \"majority_f1\": f1_score(y_test, y_pred, pos_label=0, average='binary', zero_division=0.0),\n",
        "                \"macro_f1\": f1_score(y_test, y_pred, average='macro', zero_division=0.0),\n",
        "\n",
        "                \"minority_f2\": fbeta_score(y_test, y_pred, beta=2, pos_label=1, average='binary', zero_division=0.0),\n",
        "                \"majority_f2\": fbeta_score(y_test, y_pred, beta=2, pos_label=0, average='binary', zero_division=0.0),\n",
        "                \"macro_f2\": fbeta_score(y_test, y_pred, beta=2, average='macro', zero_division=0.0),\n",
        "            }\n",
        "\n",
        "    dump_json(res, Params.BASE_LEARNER, task, name, ratio)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "JBRQe28oH-eF"
      },
      "source": [
        "# Data imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "u0j4HTKVH-eF"
      },
      "outputs": [],
      "source": [
        "from imblearn.datasets import make_imbalance\n",
        "\n",
        "IMBALANCED_DATSETS = []\n",
        "ratios = [0.5, 0.25] + [r/100 for r in range(1, 21)]\n",
        "for name, X, y in DATASETS:\n",
        "    X, y = X.copy(), y.copy()\n",
        "    c1, c2 = sorted_class_count(y)\n",
        "    IMBALANCED_DATSETS.append((name, c1/c2, X, y))\n",
        "    for ratio in sorted(ratios, reverse=True):\n",
        "        c1, c2 = sorted_class_count(y)\n",
        "        new_minority_count = int(c2 * ratio)\n",
        "        if c1 < new_minority_count:\n",
        "            continue\n",
        "        X, y = make_imbalance(X, y, sampling_strategy={0: c2, 1: new_minority_count}, random_state=Params.SEED)\n",
        "        IMBALANCED_DATSETS.append((name, ratio, X, y))\n",
        "\n",
        "for _, ratio, _, y in IMBALANCED_DATSETS:\n",
        "    c1, c2 = sorted_class_count(y)\n",
        "    assert (c1 / c2 - ratio) < 0.001\n",
        "\n",
        "[(name, ratio) for name, ratio, _, _ in IMBALANCED_DATSETS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "OVW73_dKH-eF"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "for name, ratio, X, y in IMBALANCED_DATSETS:\n",
        "    try:\n",
        "        print(name, ratio)\n",
        "        try:\n",
        "            model = load_pkl(Params.BASE_LEARNER, Task.DATA_IMBALANCE, name, ratio)\n",
        "        except:\n",
        "            model = get_AutoSklearnClassifier(X, y, Task.DATA_IMBALANCE)\n",
        "            dump_pkl(model, Params.BASE_LEARNER, Task.DATA_IMBALANCE, name, ratio)\n",
        "        validate(X, y, model, Task.DATA_IMBALANCE, name, ratio)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "fPwJE_pCH-eG"
      },
      "source": [
        "# Noisy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "XBial948H-eG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "NOISY_DATASETS = []\n",
        "noise_amount = [a / 100 for a in range(1, 11)]\n",
        "\n",
        "for name, X, y in DATASETS:\n",
        "    X, y = X.copy(), y.copy()\n",
        "\n",
        "    NOISY_DATASETS.append((name, 0, X.copy(), y.copy()))\n",
        "    indices_left = [X.index.copy() for _ in range(X.shape[1])]\n",
        "    for noise in sorted(noise_amount):\n",
        "        noise_to_add = round(X.shape[0] * noise) - (X.shape[0] - len(indices_left[0]))\n",
        "        for i, feature in enumerate(X.columns):\n",
        "            noise_indices = np.random.choice(indices_left[i], noise_to_add, replace=False)\n",
        "            if X[feature].dtype == \"float64\":\n",
        "                X.loc[noise_indices, feature] = np.random.uniform(X[feature].min(), X[feature].max(), noise_to_add)\n",
        "            elif X[feature].dtype == \"int64\":\n",
        "                X.loc[noise_indices, feature] = np.random.randint(X[feature].min(), X[feature].max()+1, noise_to_add)\n",
        "            elif X[feature].dtype == \"category\":\n",
        "                X.loc[noise_indices, feature] = np.random.choice(X[feature].unique(), noise_to_add)\n",
        "            else:\n",
        "                assert False, X[feature].dtype\n",
        "            indices_left[i] = indices_left[i].drop(noise_indices)\n",
        "\n",
        "        NOISY_DATASETS.append((name, noise, X.copy(), y.copy()))\n",
        "\n",
        "\n",
        "for i in range(1, len(NOISY_DATASETS)):\n",
        "    name, noise, X2, y2 = NOISY_DATASETS[i]\n",
        "    _, _, X, y = [\n",
        "        (name2, noise2, X2, y2)\n",
        "        for name2, noise2, X2, y2 in NOISY_DATASETS\n",
        "        if name2 == name and noise2 == 0\n",
        "    ][0]\n",
        "    for feature in X.columns:\n",
        "        assert len(X[feature]) == len(X2[feature])\n",
        "        sm = sum(a != a2 for a, a2 in zip(X[feature], X2[feature]))\n",
        "        diff = 0.075 if len(X[feature].unique()) <= 10 else 0.015\n",
        "        assert abs(sm / len(X[feature]) - noise) < diff\n",
        "\n",
        "\n",
        "[(name, noise) for name, noise, _, _ in NOISY_DATASETS]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseRemover:\n",
        "    def fit_resample(self, X, y):\n",
        "        model = get_XGBModel(X, y, \"DecisionTree\", LearnerType.REGRESSION)\n",
        "        model = model.fit(X, y)\n",
        "        X_transformed, y_transformed = X, model.predict(X)\n",
        "        return X_transformed, 0.5 <= y_transformed\n",
        "\n",
        "\n",
        "for name, noise, X, y in NOISY_DATASETS:\n",
        "    try:\n",
        "        print(name, noise)\n",
        "        X_transformed, y_transformed = NoiseRemover().fit_resample(X, y)\n",
        "        model = get_XGBModel(X_transformed, y_transformed, Params.BASE_LEARNER, LearnerType.CLASSIFICATION)\n",
        "        validate(X_transformed, y_transformed, model, Task.NOISY_DATA, name, noise)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "StZZM5SqTSlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "HTHvnwVHH-eG"
      },
      "source": [
        "# Semi-supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "6YvodmS4H-eG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "HIDDEN_DATASETS = []\n",
        "hidden_amount = [a / 100 for a in range(1, 11)]\n",
        "\n",
        "for name, X, y in DATASETS:\n",
        "    X, y = X.copy(), y.copy()\n",
        "    N = X.shape[0]\n",
        "\n",
        "    X_hidden = pd.DataFrame(np.empty((0, X.shape[1])), columns=X.columns)\n",
        "\n",
        "    HIDDEN_DATASETS.append((name, 0, X.copy(), X_hidden.copy(), y.copy()))\n",
        "    indices_left = X.index\n",
        "    for hidden in sorted(hidden_amount):\n",
        "        indices_to_hide = np.random.choice(indices_left, round(N * hidden) - (N - len(indices_left)), replace=False)\n",
        "        X_hidden = pd.concat([X_hidden, X.loc[indices_to_hide]], ignore_index=True)\n",
        "        indices_left = indices_left.drop(indices_to_hide)\n",
        "        X = X.drop(indices_to_hide)\n",
        "        y = y.drop(indices_to_hide)\n",
        "\n",
        "        HIDDEN_DATASETS.append((name, hidden, X.copy(), X_hidden.copy(), y.copy()))\n",
        "\n",
        "for name, hidden, X, X_hidden, y in HIDDEN_DATASETS:\n",
        "    assert abs(len(X_hidden) / (len(X)+len(X_hidden)) - hidden) < 0.01, f\"{len(X_hidden)}, {len(X)}, {hidden}\"\n",
        "    assert len(X) == len(y)\n",
        "    assert y.shape == (len(y), )\n",
        "    assert X.shape[1] == X_hidden.shape[1], f\"{X.shape}, {X_hidden.shape}\"\n",
        "    assert all(a==b for a, b in zip(X.columns, X_hidden.columns)), f\"{X.columns}, {X_hidden.columns}\"\n",
        "    for i in range(len(X_hidden)):\n",
        "        assert i in X_hidden.index, f\"{i}, {X_hidden.index}\"\n",
        "\n",
        "[(name, hidden) for name, hidden, _, _, _ in HIDDEN_DATASETS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {},
        "tags": [],
        "id": "exDvfXUEH-eG"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "import pandas as pd\n",
        "\n",
        "N_ESTIMATORS = 11\n",
        "ITERS = 10\n",
        "for name, hidden, X, X_hidden, y in HIDDEN_DATASETS:\n",
        "    print(name, hidden)\n",
        "    X_extra_train = pd.DataFrame(np.empty((0, X.shape[1])), columns=X.columns)\n",
        "    y_extra_train = pd.Series(np.empty(0))\n",
        "    try:\n",
        "        X, y, X_extra_train, y_extra_train = load_pkl(\n",
        "            Params.BASE_LEARNER, Task.SEMI_SUPERVISED, name, hidden\n",
        "        )\n",
        "    except:\n",
        "        num_indices_to_select = (len(X_hidden) + ITERS - 1) // ITERS\n",
        "        while len(X_hidden) != 0:\n",
        "            model = BaggingClassifier(\n",
        "                estimator=get_XGBModel(X, y, Params.BASE_LEARNER, LearnerType.CLASSIFICATION),\n",
        "                n_estimators = N_ESTIMATORS,\n",
        "                max_samples = 0.5,\n",
        "                max_features = 0.5,\n",
        "                bootstrap = True,\n",
        "                bootstrap_features = True,\n",
        "                n_jobs=-1,\n",
        "                random_state = Params.SEED\n",
        "            )\n",
        "            X_train = pd.concat([X, X_extra_train], ignore_index=True)\n",
        "            if len(y_extra_train) != 0:\n",
        "                y_train = pd.concat([y, y_extra_train], ignore_index=True)\n",
        "            else:\n",
        "                y_train = y\n",
        "            model = model.fit(X_train, y_train.to_numpy().ravel())\n",
        "            y_prob = model.predict_proba(X_hidden)[:, 0].flatten()\n",
        "            max_indices = np.argsort(np.maximum(y_prob, 1 - y_prob))[::-1][:num_indices_to_select]\n",
        "            X_extra_train = pd.concat([X_extra_train, pd.DataFrame(X_hidden.iloc[max_indices])], ignore_index=True)\n",
        "            if len(y_extra_train) != 0:\n",
        "                y_extra_train = pd.concat([y_extra_train, pd.Series(0.5 <= y_prob[max_indices])], ignore_index=True)\n",
        "            else:\n",
        "                y_extra_train = pd.Series(0.5 <= y_prob[max_indices])\n",
        "            X_hidden = X_hidden.drop(X_hidden.index[max_indices])\n",
        "        dump_pkl((X, y, X_extra_train, y_extra_train), Params.BASE_LEARNER, Task.SEMI_SUPERVISED, name, hidden)\n",
        "\n",
        "    model = get_XGBModel(X, y, Params.BASE_LEARNER, LearnerType.CLASSIFICATION)\n",
        "    validate(X, y, model, Task.SEMI_SUPERVISED, name, hidden, extra=(X_extra_train, y_extra_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5k4tvUZH-eH"
      },
      "source": [
        "# Inadequate features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eOgX9nV3H-eH"
      },
      "outputs": [],
      "source": [
        "for name, X, y in DATASETS:\n",
        "    print(name)\n",
        "    try:\n",
        "        model = load_pkl(Params.BASE_LEARNER, Task.FEATURE_INADEQUACY, name, None)\n",
        "    except:\n",
        "        model = get_AutoSklearnClassifier(X, y, task=Task.FEATURE_INADEQUACY)\n",
        "        dump_pkl(model, Params.BASE_LEARNER, Task.FEATURE_INADEQUACY, name, None)\n",
        "    validate(X, y, model, Task.FEATURE_INADEQUACY, name, None)"
      ]
    }
  ]
}
